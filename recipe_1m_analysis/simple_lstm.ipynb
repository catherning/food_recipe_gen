{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"D:\\\\Documents\\\\food_recipe_gen\\\\recipe_1m_analysis\"\n",
    "files = [\"allingrs_count.pkl\",\"allwords_count.pkl\",\"recipe1m_test.pkl\",\"recipe1m_vocab_ingrs.pkl\",\"recipe1m_vocab_toks.pkl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(folder,\"data\",files[2]),'rb') as f:\n",
    "    data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text=[]\n",
    "full_text=[]\n",
    "for i,recipe in enumerate(data):\n",
    "    raw_text.append(' '.join(recipe[\"instructions\"]))\n",
    "    full_text.extend(' '.join(recipe[\"instructions\"]))\n",
    "    if i>=3000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars=set()\n",
    "for recipe in raw_text:\n",
    "    chars=chars.union(set(recipe))\n",
    "chars=sorted(list(chars))\n",
    "chars.append(\"<pad>\")\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  3001\n",
      "Total Vocab:  60\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  2901\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = full_text[i:i + seq_length]\n",
    "    seq_out = full_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(os.path.join(folder,\"weights\",filepath), monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "2901/2901 [==============================] - 13s 5ms/step - loss: 1.0847\n",
      "\n",
      "Epoch 00001: loss improved from 1.19499 to 1.08465, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-01-1.0847.hdf5\n",
      "Epoch 2/40\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 1.0558\n",
      "\n",
      "Epoch 00002: loss improved from 1.08465 to 1.05580, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-02-1.0558.hdf5\n",
      "Epoch 3/40\n",
      "2901/2901 [==============================] - 10s 4ms/step - loss: 1.0225\n",
      "\n",
      "Epoch 00003: loss improved from 1.05580 to 1.02252, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-03-1.0225.hdf5\n",
      "Epoch 4/40\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 0.9549\n",
      "\n",
      "Epoch 00004: loss improved from 1.02252 to 0.95491, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-04-0.9549.hdf5\n",
      "Epoch 5/40\n",
      "2901/2901 [==============================] - 10s 4ms/step - loss: 0.9209\n",
      "\n",
      "Epoch 00005: loss improved from 0.95491 to 0.92094, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-05-0.9209.hdf5\n",
      "Epoch 6/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 0.8747\n",
      "\n",
      "Epoch 00006: loss improved from 0.92094 to 0.87473, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-06-0.8747.hdf5\n",
      "Epoch 7/40\n",
      "2901/2901 [==============================] - 10s 4ms/step - loss: 0.8599\n",
      "\n",
      "Epoch 00007: loss improved from 0.87473 to 0.85989, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-07-0.8599.hdf5\n",
      "Epoch 8/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 1.3341\n",
      "\n",
      "Epoch 00008: loss did not improve from 0.85989\n",
      "Epoch 9/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 1.3636\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.85989\n",
      "Epoch 10/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 1.0236\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.85989\n",
      "Epoch 11/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 0.8695\n",
      "\n",
      "Epoch 00011: loss did not improve from 0.85989\n",
      "Epoch 12/40\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 0.7709\n",
      "\n",
      "Epoch 00012: loss improved from 0.85989 to 0.77087, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-12-0.7709.hdf5\n",
      "Epoch 13/40\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 0.7403\n",
      "\n",
      "Epoch 00013: loss improved from 0.77087 to 0.74032, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-13-0.7403.hdf5\n",
      "Epoch 14/40\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 0.7046\n",
      "\n",
      "Epoch 00014: loss improved from 0.74032 to 0.70457, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-14-0.7046.hdf5\n",
      "Epoch 15/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 0.6468\n",
      "\n",
      "Epoch 00015: loss improved from 0.70457 to 0.64676, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-15-0.6468.hdf5\n",
      "Epoch 16/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 0.6245\n",
      "\n",
      "Epoch 00016: loss improved from 0.64676 to 0.62447, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-16-0.6245.hdf5\n",
      "Epoch 17/40\n",
      "2901/2901 [==============================] - 10s 4ms/step - loss: 0.5848\n",
      "\n",
      "Epoch 00017: loss improved from 0.62447 to 0.58485, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-17-0.5848.hdf5\n",
      "Epoch 18/40\n",
      "2901/2901 [==============================] - 10s 4ms/step - loss: 0.5491\n",
      "\n",
      "Epoch 00018: loss improved from 0.58485 to 0.54909, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-18-0.5491.hdf5\n",
      "Epoch 19/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 0.5274\n",
      "\n",
      "Epoch 00019: loss improved from 0.54909 to 0.52738, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-19-0.5274.hdf5\n",
      "Epoch 20/40\n",
      "2901/2901 [==============================] - 16s 5ms/step - loss: 0.5257\n",
      "\n",
      "Epoch 00020: loss improved from 0.52738 to 0.52572, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-20-0.5257.hdf5\n",
      "Epoch 21/40\n",
      "2901/2901 [==============================] - 13s 4ms/step - loss: 0.6225\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.52572\n",
      "Epoch 22/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 0.5556\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.52572\n",
      "Epoch 23/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 0.4909\n",
      "\n",
      "Epoch 00023: loss improved from 0.52572 to 0.49089, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-23-0.4909.hdf5\n",
      "Epoch 24/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 0.4422\n",
      "\n",
      "Epoch 00024: loss improved from 0.49089 to 0.44216, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-24-0.4422.hdf5\n",
      "Epoch 25/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 0.4289\n",
      "\n",
      "Epoch 00025: loss improved from 0.44216 to 0.42891, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-25-0.4289.hdf5\n",
      "Epoch 26/40\n",
      "2901/2901 [==============================] - 12s 4ms/step - loss: 0.3927\n",
      "\n",
      "Epoch 00026: loss improved from 0.42891 to 0.39267, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-26-0.3927.hdf5\n",
      "Epoch 27/40\n",
      "2901/2901 [==============================] - 14s 5ms/step - loss: 0.3891\n",
      "\n",
      "Epoch 00027: loss improved from 0.39267 to 0.38906, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-27-0.3891.hdf5\n",
      "Epoch 28/40\n",
      "2901/2901 [==============================] - 14s 5ms/step - loss: 0.4175\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.38906\n",
      "Epoch 29/40\n",
      "2901/2901 [==============================] - 12s 4ms/step - loss: 0.3744\n",
      "\n",
      "Epoch 00029: loss improved from 0.38906 to 0.37436, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-29-0.3744.hdf5\n",
      "Epoch 30/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 0.3529\n",
      "\n",
      "Epoch 00030: loss improved from 0.37436 to 0.35289, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-30-0.3529.hdf5\n",
      "Epoch 31/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 0.3197\n",
      "\n",
      "Epoch 00031: loss improved from 0.35289 to 0.31972, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-31-0.3197.hdf5\n",
      "Epoch 32/40\n",
      "2901/2901 [==============================] - 12s 4ms/step - loss: 0.3008\n",
      "\n",
      "Epoch 00032: loss improved from 0.31972 to 0.30082, saving model to D:\\Documents\\food_recipe_gen\\recipe_1m_analysis\\weights\\weights-improvement-32-0.3008.hdf5\n",
      "Epoch 33/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 1.2520\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.30082\n",
      "Epoch 34/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 4.3344\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.30082\n",
      "Epoch 35/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 3.9949\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.30082\n",
      "Epoch 36/40\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 3.3358\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.30082\n",
      "Epoch 37/40\n",
      "2901/2901 [==============================] - 12s 4ms/step - loss: 3.1052\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.30082\n",
      "Epoch 38/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 3.0248\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.30082\n",
      "Epoch 39/40\n",
      "2901/2901 [==============================] - 13s 5ms/step - loss: 2.9882\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.30082\n",
      "Epoch 40/40\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 2.9583\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.30082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x280770a7d88>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=40, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# load the network weights\n",
    "list_of_files = glob.glob(os.path.join(folder,\"weights\",\"*.hdf5\")) # * means all if need specific format then *.csv\n",
    "latest_file = max(list_of_files, key=os.path.getctime)\n",
    "model2.load_weights(latest_file)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Seed:\n",
      "\" ved from the heat. in a large bowl, whisk together the eggs, cheese, and salt. in a large pot of boi \"\n",
      "ling, salted water, cook the pasta until just mene aarec do the aanis ritt  br ntt  laonn codep and sntte mitia bustt ant cntoer en rlir so tlen isste darie. add tee parsnee and toss just until mixed. salt and mep enr pate mesische tt t soeirbee tir rose. seaase, gnd ialle iilea aaset tn the connst,an  ohopl ff innulooooo  iiint to room oomm beooee srmvi.gi ahdorb doole bnt tott rariaa aodlt the  aslz, and tur pff.n(ot sogetoer the tod er rest batte. iddaee  aass,,and pepper. simmer un il med mu\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "#start = 4\n",
    "pattern = dataX[start]\n",
    "print(len(pattern))\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "output=[]\n",
    "for i in range(500):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model2.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    output.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(''.join(output))\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingrs=\"beef mint onion garlic pasta soja tomato french main dish\"\n",
    "patt=[char_to_int[value] for value in ingrs]\n",
    "patt = (100 * [char_to_int[\"<pad>\"]] + patt)[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pisin  ra astorr oith b meieee  add nge oadi aa blsllerett iite the wiote satt.. araat  lo  map mer nnte booadne  to s tto  bro  thl mapin 1f minutes. * if you like a  weeetee corle  soou  n dppepp pote a mexee. and ygf gadi aantaee toet theetient toe tog  wd aut brans coeek yogewe,cater and but bran. cream toee pream ao text ra chelree over the meins  add th sos tige m oear settl  or atl llay  rgea aooorr  b minuts  aad the iarli  wile, and pepper. simmer until the wine is reduced to 2 tbsp, ab\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "output=[]\n",
    "for i in range(500):\n",
    "    x = numpy.reshape(patt, (1, len(patt), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model2.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in patt]\n",
    "    output.append(result)\n",
    "    patt.append(index)\n",
    "    patt = patt[1:len(patt)]\n",
    "print(''.join(output))\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
