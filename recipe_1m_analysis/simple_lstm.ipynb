{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Applications\\Anaconda3\\envs\\pandas_analysis\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"D:\\\\Documents\\\\food_recipe_gen\\\\recipe_1m_analysis\"\n",
    "files = [\"allingrs_count.pkl\",\"allwords_count.pkl\",\"recipe1m_test.pkl\",\"recipe1m_vocab_ingrs.pkl\",\"recipe1m_vocab_toks.pkl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(folder,\"data\",files[2]),'rb') as f:\n",
    "    data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text=[]\n",
    "full_text=[]\n",
    "for i,recipe in enumerate(data):\n",
    "    raw_text.append(' '.join(recipe[\"instructions\"]))\n",
    "    full_text.extend(' '.join(recipe[\"instructions\"]))\n",
    "    if i>=3000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars=set()\n",
    "for recipe in raw_text:\n",
    "    chars=chars.union(set(recipe))\n",
    "chars=sorted(list(chars))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  3001\n",
      "Total Vocab:  59\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  2901\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = full_text[i:i + seq_length]\n",
    "    seq_out = full_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(os.path.join(folder,\"weights\",filepath), monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 2.6549\n",
      "\n",
      "Epoch 00001: loss improved from 2.66676 to 2.65489, saving model to weights-improvement-01-2.6549.hdf5\n",
      "Epoch 2/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.6447\n",
      "\n",
      "Epoch 00002: loss improved from 2.65489 to 2.64466, saving model to weights-improvement-02-2.6447.hdf5\n",
      "Epoch 3/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.6338\n",
      "\n",
      "Epoch 00003: loss improved from 2.64466 to 2.63380, saving model to weights-improvement-03-2.6338.hdf5\n",
      "Epoch 4/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.6174\n",
      "\n",
      "Epoch 00004: loss improved from 2.63380 to 2.61739, saving model to weights-improvement-04-2.6174.hdf5\n",
      "Epoch 5/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.6041\n",
      "\n",
      "Epoch 00005: loss improved from 2.61739 to 2.60405, saving model to weights-improvement-05-2.6041.hdf5\n",
      "Epoch 6/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.6095\n",
      "\n",
      "Epoch 00006: loss did not improve from 2.60405\n",
      "Epoch 7/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.5922\n",
      "\n",
      "Epoch 00007: loss improved from 2.60405 to 2.59221, saving model to weights-improvement-07-2.5922.hdf5\n",
      "Epoch 8/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.5609\n",
      "\n",
      "Epoch 00008: loss improved from 2.59221 to 2.56088, saving model to weights-improvement-08-2.5609.hdf5\n",
      "Epoch 9/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.5511\n",
      "\n",
      "Epoch 00009: loss improved from 2.56088 to 2.55110, saving model to weights-improvement-09-2.5511.hdf5\n",
      "Epoch 10/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.5387\n",
      "\n",
      "Epoch 00010: loss improved from 2.55110 to 2.53866, saving model to weights-improvement-10-2.5387.hdf5\n",
      "Epoch 11/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.5395\n",
      "\n",
      "Epoch 00011: loss did not improve from 2.53866\n",
      "Epoch 12/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.4855\n",
      "\n",
      "Epoch 00012: loss improved from 2.53866 to 2.48546, saving model to weights-improvement-12-2.4855.hdf5\n",
      "Epoch 13/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 2.4752\n",
      "\n",
      "Epoch 00013: loss improved from 2.48546 to 2.47525, saving model to weights-improvement-13-2.4752.hdf5\n",
      "Epoch 14/50\n",
      "2901/2901 [==============================] - 10s 4ms/step - loss: 2.4620\n",
      "\n",
      "Epoch 00014: loss improved from 2.47525 to 2.46203, saving model to weights-improvement-14-2.4620.hdf5\n",
      "Epoch 15/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 2.4354\n",
      "\n",
      "Epoch 00015: loss improved from 2.46203 to 2.43536, saving model to weights-improvement-15-2.4354.hdf5\n",
      "Epoch 16/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 2.4441\n",
      "\n",
      "Epoch 00016: loss did not improve from 2.43536\n",
      "Epoch 17/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.3951\n",
      "\n",
      "Epoch 00017: loss improved from 2.43536 to 2.39511, saving model to weights-improvement-17-2.3951.hdf5\n",
      "Epoch 18/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 2.3496\n",
      "\n",
      "Epoch 00018: loss improved from 2.39511 to 2.34958, saving model to weights-improvement-18-2.3496.hdf5\n",
      "Epoch 19/50\n",
      "2901/2901 [==============================] - 13s 4ms/step - loss: 2.3275\n",
      "\n",
      "Epoch 00019: loss improved from 2.34958 to 2.32749, saving model to weights-improvement-19-2.3275.hdf5\n",
      "Epoch 20/50\n",
      "2901/2901 [==============================] - 13s 5ms/step - loss: 2.3070\n",
      "\n",
      "Epoch 00020: loss improved from 2.32749 to 2.30703, saving model to weights-improvement-20-2.3070.hdf5\n",
      "Epoch 21/50\n",
      "2901/2901 [==============================] - 13s 5ms/step - loss: 2.2553\n",
      "\n",
      "Epoch 00021: loss improved from 2.30703 to 2.25531, saving model to weights-improvement-21-2.2553.hdf5\n",
      "Epoch 22/50\n",
      "2901/2901 [==============================] - 10s 4ms/step - loss: 2.2233\n",
      "\n",
      "Epoch 00022: loss improved from 2.25531 to 2.22334, saving model to weights-improvement-22-2.2233.hdf5\n",
      "Epoch 23/50\n",
      "2901/2901 [==============================] - 10s 4ms/step - loss: 2.1738\n",
      "\n",
      "Epoch 00023: loss improved from 2.22334 to 2.17385, saving model to weights-improvement-23-2.1738.hdf5\n",
      "Epoch 24/50\n",
      "2901/2901 [==============================] - 10s 4ms/step - loss: 2.1428\n",
      "\n",
      "Epoch 00024: loss improved from 2.17385 to 2.14280, saving model to weights-improvement-24-2.1428.hdf5\n",
      "Epoch 25/50\n",
      "2901/2901 [==============================] - 10s 4ms/step - loss: 2.1002\n",
      "\n",
      "Epoch 00025: loss improved from 2.14280 to 2.10015, saving model to weights-improvement-25-2.1002.hdf5\n",
      "Epoch 26/50\n",
      "2901/2901 [==============================] - 12s 4ms/step - loss: 2.0643\n",
      "\n",
      "Epoch 00026: loss improved from 2.10015 to 2.06430, saving model to weights-improvement-26-2.0643.hdf5\n",
      "Epoch 27/50\n",
      "2901/2901 [==============================] - 12s 4ms/step - loss: 2.0305\n",
      "\n",
      "Epoch 00027: loss improved from 2.06430 to 2.03054, saving model to weights-improvement-27-2.0305.hdf5\n",
      "Epoch 28/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 1.9759\n",
      "\n",
      "Epoch 00028: loss improved from 2.03054 to 1.97592, saving model to weights-improvement-28-1.9759.hdf5\n",
      "Epoch 29/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 1.9578\n",
      "\n",
      "Epoch 00029: loss improved from 1.97592 to 1.95783, saving model to weights-improvement-29-1.9578.hdf5\n",
      "Epoch 30/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 1.8850\n",
      "\n",
      "Epoch 00030: loss improved from 1.95783 to 1.88499, saving model to weights-improvement-30-1.8850.hdf5\n",
      "Epoch 31/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 1.8254\n",
      "\n",
      "Epoch 00031: loss improved from 1.88499 to 1.82541, saving model to weights-improvement-31-1.8254.hdf5\n",
      "Epoch 32/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 1.7772\n",
      "\n",
      "Epoch 00032: loss improved from 1.82541 to 1.77717, saving model to weights-improvement-32-1.7772.hdf5\n",
      "Epoch 33/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 1.7428\n",
      "\n",
      "Epoch 00033: loss improved from 1.77717 to 1.74285, saving model to weights-improvement-33-1.7428.hdf5\n",
      "Epoch 34/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 1.6897\n",
      "\n",
      "Epoch 00034: loss improved from 1.74285 to 1.68974, saving model to weights-improvement-34-1.6897.hdf5\n",
      "Epoch 35/50\n",
      "2901/2901 [==============================] - 10s 4ms/step - loss: 1.6362\n",
      "\n",
      "Epoch 00035: loss improved from 1.68974 to 1.63618, saving model to weights-improvement-35-1.6362.hdf5\n",
      "Epoch 36/50\n",
      "2901/2901 [==============================] - 12s 4ms/step - loss: 1.6023\n",
      "\n",
      "Epoch 00036: loss improved from 1.63618 to 1.60226, saving model to weights-improvement-36-1.6023.hdf5\n",
      "Epoch 37/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 1.5598\n",
      "\n",
      "Epoch 00037: loss improved from 1.60226 to 1.55982, saving model to weights-improvement-37-1.5598.hdf5\n",
      "Epoch 38/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 1.4813\n",
      "\n",
      "Epoch 00038: loss improved from 1.55982 to 1.48131, saving model to weights-improvement-38-1.4813.hdf5\n",
      "Epoch 39/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 1.4347\n",
      "\n",
      "Epoch 00039: loss improved from 1.48131 to 1.43473, saving model to weights-improvement-39-1.4347.hdf5\n",
      "Epoch 40/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 2.0179\n",
      "\n",
      "Epoch 00040: loss did not improve from 1.43473\n",
      "Epoch 41/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 3.6850\n",
      "\n",
      "Epoch 00041: loss did not improve from 1.43473\n",
      "Epoch 42/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 3.2375\n",
      "\n",
      "Epoch 00042: loss did not improve from 1.43473\n",
      "Epoch 43/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 3.1223\n",
      "\n",
      "Epoch 00043: loss did not improve from 1.43473\n",
      "Epoch 44/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 3.0912\n",
      "\n",
      "Epoch 00044: loss did not improve from 1.43473\n",
      "Epoch 45/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 3.0352\n",
      "\n",
      "Epoch 00045: loss did not improve from 1.43473\n",
      "Epoch 46/50\n",
      "2901/2901 [==============================] - 11s 4ms/step - loss: 2.9737\n",
      "\n",
      "Epoch 00046: loss did not improve from 1.43473\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2901/2901 [==============================] - 11s 4ms/step - loss: 2.9584\n",
      "\n",
      "Epoch 00047: loss did not improve from 1.43473\n",
      "Epoch 48/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.9727\n",
      "\n",
      "Epoch 00048: loss did not improve from 1.43473\n",
      "Epoch 49/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 2.9837\n",
      "\n",
      "Epoch 00049: loss did not improve from 1.43473\n",
      "Epoch 50/50\n",
      "2901/2901 [==============================] - 10s 3ms/step - loss: 3.0016\n",
      "\n",
      "Epoch 00050: loss did not improve from 1.43473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2309bf5d048>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# load the network weights\n",
    "list_of_files = glob.glob(os.path.join(folder,\"weights\",\"*.hdf5\")) # * means all if need specific format then *.csv\n",
    "latest_file = max(list_of_files, key=os.path.getctime)\n",
    "model2.load_weights(latest_file)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  pepper, lemon zest and juice. combine the marinade with the beans and chopped parsley in a medium b \"\n",
      "owl. cover and set mrrin tett  ams aod sa the aod iut setil mixin didte and mette andin the thst aadiie  aa ailoe fon nner  od tte poronr  bnmnnt, sdl aanoee pi mester  nd ther fool  adsii  ad tue panin rn t sid der ses sest re ihsr menic  adiite  add tgg  add tve  adinu c minuee  fmm nee ior miiteee tver teees.eh the ror mnd shete aadii ceea phoam andil tortl pixt  adsit 4 minutes. aad the gar chill soel fers mitil prrtr mint  addit  od the parinr  nn nnt  ids cud pexter  idmnn sis  addin  nd t\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "output=[]\n",
    "for i in range(500):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model2.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    output.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(''.join(output))\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
