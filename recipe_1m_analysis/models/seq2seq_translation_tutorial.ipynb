{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "from functools import reduce\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"D:\\\\Documents\\\\food_recipe_gen\\\\recipe_1m_analysis\")\n",
    "\n",
    "from utils import Vocabulary, FOLDER_PATH, DATA_FILES \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the character encoding used in the character-level RNN\n",
    "tutorials, we will be representing each word in a language as a one-hot\n",
    "vector, or giant vector of zeros except for a single one (at the index\n",
    "of the word). Compared to the dozens of characters that might exist in a\n",
    "language, there are many many more words, so the encoding vector is much\n",
    "larger. We will however cheat a bit and trim the data to only use a few\n",
    "thousand words per language.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/word-encoding.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a unique index per word to use as the inputs and targets of\n",
    "the networks later. To keep track of all this we will use a helper class\n",
    "called ``Lang`` which has word → index (``word2index``) and index → word\n",
    "(``index2word``) dictionaries, as well as a count of each word\n",
    "``word2count`` to use to later replace rare words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['allingrs_count.pkl',\n",
       " 'allwords_count.pkl',\n",
       " 'recipe1m_test.pkl',\n",
       " 'recipe1m_vocab_ingrs.pkl',\n",
       " 'recipe1m_vocab_toks.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(FOLDER_PATH,DATA_FILES[3]),'rb') as f:\n",
    "    vocab_ingrs=pickle.load(f)\n",
    "    \n",
    "with open(os.path.join(FOLDER_PATH,DATA_FILES[4]),'rb') as f:\n",
    "    vocab_tokens=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18745\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(FOLDER_PATH,DATA_FILES[2]),'rb') as f:\n",
    "    data=pickle.load(f)\n",
    "\n",
    "pairs=[]\n",
    "for recipe in data:\n",
    "    pairs.append([recipe[\"ingredients\"],recipe[\"tokenized\"]])\n",
    "\n",
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length=0\n",
    "for i,pair in enumerate(pairs):\n",
    "#     length_t=reduce(lambda x,y:len(x)+len(y), pair[1])\n",
    "    length_t=0\n",
    "    for sent in pair[1]:\n",
    "        length_t+=len(sent)\n",
    "    \n",
    "    if length_t>length:\n",
    "        length=length_t\n",
    "\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs[16826]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14724\n"
     ]
    }
   ],
   "source": [
    "def filterPair(p):\n",
    "    length=0\n",
    "    for ingr in p[0]:\n",
    "        if ingr not in vocab_ingrs.word2idx:\n",
    "            return False\n",
    "        \n",
    "    for sent in p[1]:\n",
    "        \n",
    "        for word in sent:\n",
    "            # TODO check how steps tokenized ? Put into vocab ???\n",
    "            if word not in vocab_tokens.word2idx:\n",
    "                return False\n",
    "        length+=len(sent)\n",
    "    \n",
    "    return length < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "pairs = filterPairs(pairs)\n",
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_ingrs.word2idx['quahogs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
    "  limitation by using a relative position approach. Read about \"local\n",
    "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
    "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
    "\n",
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2idx(vocab, sentence):\n",
    "    return [vocab.word2idx[word] for word in sentence]\n",
    "\n",
    "\n",
    "def tensorFromSentence(vocab, sentence,instructions=False):\n",
    "    if instructions:\n",
    "        indexes=[]\n",
    "        for sent in sentence:\n",
    "            indexes.extend(list2idx(vocab, sent))\n",
    "    else:\n",
    "        indexes = list2idx(vocab, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(vocab_ingrs, pair[0])\n",
    "    target_tensor = tensorFromSentence(vocab_tokens, pair[1],instructions=True)\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "# for i,pair in enumerate(pairs):\n",
    "#     try:\n",
    "#         tensorsFromPair(pair)\n",
    "#     except KeyError as e:\n",
    "#         print(e)\n",
    "#         print(i)\n",
    "#         print(pair)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
    "black cat\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Decoder**\n",
    "\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and\n",
    "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
    "token, and the first hidden state is the context vector (the encoder's\n",
    "last hidden state).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to train and observe the results of this model, but to\n",
    "save space we'll be going straight for the gold and introducing the\n",
    "Attention Mechanism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Decoder**\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    ".. figure:: https://i.imgur.com/1152PYf.png\n",
    "   :alt:\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf>`__.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH, attention=False):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        if attention:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                loss += criterion(decoder_output, target_tensor[di])\n",
    "                decoder_input = target_tensor[di]  # Teacher forcing\n",
    "        else:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "                loss += criterion(decoder_output, target_tensor[di])\n",
    "                decoder_input = target_tensor[di]  # Teacher forcing\n",
    "            \n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        if attention:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "                loss += criterion(decoder_output, target_tensor[di])\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "        else:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "                loss += criterion(decoder_output, target_tensor[di])\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01,attention=True):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion,attention=attention)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results\n",
    "----------------\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(vocab_ingrs, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(vocab_tokens.idx2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
    "reasonable results.\n",
    "\n",
    ".. Note::\n",
    "   If you run this notebook you can train, interrupt the kernel,\n",
    "   evaluate, and continue training later. Comment out the lines where the\n",
    "   encoder and decoder are initialized and run ``trainIters`` again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 27s (- 144m 29s) (100 1%) 20.5818\n",
      "2m 49s (- 138m 23s) (200 2%) 25.0482\n",
      "4m 13s (- 136m 36s) (300 3%) 32.8347\n",
      "5m 37s (- 135m 5s) (400 4%) 36.4157\n",
      "6m 56s (- 131m 54s) (500 5%) 37.1037\n",
      "8m 22s (- 131m 12s) (600 6%) 42.1024\n",
      "9m 47s (- 130m 10s) (700 7%) 43.1715\n",
      "11m 8s (- 128m 12s) (800 8%) 48.5861\n",
      "12m 31s (- 126m 39s) (900 9%) 49.4105\n",
      "14m 3s (- 126m 31s) (1000 10%) 50.4300\n",
      "15m 37s (- 126m 21s) (1100 11%) 51.5683\n",
      "16m 57s (- 124m 24s) (1200 12%) 45.9525\n",
      "18m 37s (- 124m 40s) (1300 13%) 60.4985\n",
      "20m 0s (- 122m 52s) (1400 14%) 51.0973\n",
      "21m 36s (- 122m 27s) (1500 15%) 53.4614\n",
      "23m 2s (- 120m 58s) (1600 16%) 50.1443\n",
      "24m 31s (- 119m 42s) (1700 17%) 52.3147\n",
      "25m 58s (- 118m 18s) (1800 18%) 56.0034\n",
      "27m 20s (- 116m 34s) (1900 19%) 49.9412\n",
      "28m 37s (- 114m 31s) (2000 20%) 49.9150\n",
      "30m 4s (- 113m 9s) (2100 21%) 53.6711\n",
      "31m 25s (- 111m 26s) (2200 22%) 49.7835\n",
      "32m 53s (- 110m 6s) (2300 23%) 57.0558\n",
      "34m 18s (- 108m 38s) (2400 24%) 56.2472\n",
      "35m 42s (- 107m 7s) (2500 25%) 55.0453\n",
      "37m 5s (- 105m 35s) (2600 26%) 60.2001\n",
      "38m 31s (- 104m 8s) (2700 27%) 59.5025\n",
      "39m 56s (- 102m 42s) (2800 28%) 54.7209\n",
      "41m 25s (- 101m 25s) (2900 28%) 57.6889\n",
      "42m 52s (- 100m 3s) (3000 30%) 57.1217\n",
      "44m 22s (- 98m 46s) (3100 31%) 62.5641\n",
      "45m 47s (- 97m 18s) (3200 32%) 58.8540\n",
      "47m 18s (- 96m 3s) (3300 33%) 55.4264\n",
      "48m 35s (- 94m 20s) (3400 34%) 49.9850\n",
      "49m 59s (- 92m 51s) (3500 35%) 56.5058\n",
      "51m 24s (- 91m 23s) (3600 36%) 57.8563\n",
      "52m 48s (- 89m 54s) (3700 37%) 60.7801\n",
      "54m 20s (- 88m 39s) (3800 38%) 61.9719\n",
      "55m 52s (- 87m 24s) (3900 39%) 54.8061\n",
      "57m 22s (- 86m 3s) (4000 40%) 56.3483\n",
      "58m 46s (- 84m 34s) (4100 41%) 57.5434\n",
      "60m 7s (- 83m 2s) (4200 42%) 56.3081\n",
      "61m 29s (- 81m 31s) (4300 43%) 60.9368\n",
      "62m 52s (- 80m 0s) (4400 44%) 54.9090\n",
      "64m 12s (- 78m 28s) (4500 45%) 58.5269\n",
      "65m 39s (- 77m 4s) (4600 46%) 58.8873\n",
      "67m 4s (- 75m 38s) (4700 47%) 63.6986\n",
      "68m 33s (- 74m 15s) (4800 48%) 62.4553\n",
      "69m 58s (- 72m 49s) (4900 49%) 56.3722\n",
      "71m 21s (- 71m 21s) (5000 50%) 51.3419\n",
      "72m 45s (- 69m 54s) (5100 51%) 57.7316\n",
      "74m 11s (- 68m 29s) (5200 52%) 61.4291\n",
      "75m 41s (- 67m 7s) (5300 53%) 56.6479\n",
      "77m 4s (- 65m 39s) (5400 54%) 59.9080\n",
      "78m 31s (- 64m 15s) (5500 55%) 60.9332\n",
      "79m 56s (- 62m 48s) (5600 56%) 58.8729\n",
      "81m 20s (- 61m 21s) (5700 56%) 59.4523\n",
      "82m 42s (- 59m 53s) (5800 57%) 57.0820\n",
      "84m 5s (- 58m 26s) (5900 59%) 50.6066\n",
      "85m 29s (- 56m 59s) (6000 60%) 57.4382\n",
      "86m 51s (- 55m 31s) (6100 61%) 59.5602\n",
      "88m 18s (- 54m 7s) (6200 62%) 59.9924\n",
      "89m 44s (- 52m 42s) (6300 63%) 59.4527\n",
      "91m 9s (- 51m 16s) (6400 64%) 64.3025\n",
      "92m 33s (- 49m 50s) (6500 65%) 58.6625\n",
      "93m 58s (- 48m 24s) (6600 66%) 59.2971\n",
      "95m 19s (- 46m 57s) (6700 67%) 57.0331\n",
      "96m 44s (- 45m 31s) (6800 68%) 58.3189\n",
      "98m 5s (- 44m 4s) (6900 69%) 60.8749\n",
      "99m 26s (- 42m 37s) (7000 70%) 59.9753\n",
      "100m 51s (- 41m 11s) (7100 71%) 60.8777\n",
      "102m 16s (- 39m 46s) (7200 72%) 62.2782\n",
      "103m 32s (- 38m 17s) (7300 73%) 54.2598\n",
      "104m 56s (- 36m 52s) (7400 74%) 64.6586\n",
      "106m 25s (- 35m 28s) (7500 75%) 65.2378\n",
      "107m 51s (- 34m 3s) (7600 76%) 64.3948\n",
      "109m 16s (- 32m 38s) (7700 77%) 61.2492\n",
      "110m 39s (- 31m 12s) (7800 78%) 59.2964\n",
      "112m 3s (- 29m 47s) (7900 79%) 55.4526\n",
      "113m 29s (- 28m 22s) (8000 80%) 66.4366\n",
      "114m 53s (- 26m 56s) (8100 81%) 56.5092\n",
      "116m 11s (- 25m 30s) (8200 82%) 54.1103\n",
      "117m 32s (- 24m 4s) (8300 83%) 55.6891\n",
      "119m 4s (- 22m 40s) (8400 84%) 67.0278\n",
      "120m 29s (- 21m 15s) (8500 85%) 61.5655\n",
      "121m 59s (- 19m 51s) (8600 86%) 63.0055\n",
      "123m 31s (- 18m 27s) (8700 87%) 62.3679\n",
      "124m 54s (- 17m 1s) (8800 88%) 63.5164\n",
      "126m 20s (- 15m 36s) (8900 89%) 59.6964\n",
      "127m 48s (- 14m 12s) (9000 90%) 60.1106\n",
      "129m 14s (- 12m 46s) (9100 91%) 63.9118\n",
      "130m 33s (- 11m 21s) (9200 92%) 59.0758\n",
      "131m 52s (- 9m 55s) (9300 93%) 52.7335\n",
      "133m 16s (- 8m 30s) (9400 94%) 58.3753\n",
      "134m 40s (- 7m 5s) (9500 95%) 64.3879\n",
      "136m 7s (- 5m 40s) (9600 96%) 58.0484\n",
      "137m 30s (- 4m 15s) (9700 97%) 59.3641\n",
      "138m 55s (- 2m 50s) (9800 98%) 60.1932\n",
      "140m 20s (- 1m 25s) (9900 99%) 65.4317\n",
      "141m 44s (- 0m 0s) (10000 100%) 65.6639\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(len(vocab_ingrs.idx2word.keys()), hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, len(vocab_tokens.idx2word.keys()), dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 5000, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 7s (- 54m 51s) (100 2%) 35.4625\n",
      "2m 14s (- 53m 42s) (200 4%) 47.4241\n",
      "3m 27s (- 54m 18s) (300 6%) 56.7911\n",
      "4m 41s (- 53m 51s) (400 8%) 58.8278\n",
      "5m 52s (- 52m 54s) (500 10%) 65.7322\n",
      "7m 6s (- 52m 6s) (600 12%) 68.7951\n",
      "8m 22s (- 51m 24s) (700 14%) 76.7185\n",
      "9m 35s (- 50m 19s) (800 16%) 70.3571\n",
      "10m 43s (- 48m 52s) (900 18%) 65.3358\n",
      "11m 51s (- 47m 25s) (1000 20%) 71.5617\n",
      "13m 1s (- 46m 9s) (1100 22%) 64.5580\n",
      "14m 12s (- 44m 58s) (1200 24%) 66.0643\n",
      "15m 21s (- 43m 41s) (1300 26%) 67.3903\n",
      "16m 29s (- 42m 24s) (1400 28%) 66.9448\n",
      "17m 37s (- 41m 7s) (1500 30%) 67.3278\n",
      "18m 52s (- 40m 6s) (1600 32%) 70.7842\n",
      "20m 7s (- 39m 4s) (1700 34%) 77.2261\n",
      "21m 12s (- 37m 42s) (1800 36%) 67.4372\n",
      "22m 21s (- 36m 29s) (1900 38%) 67.6995\n",
      "23m 28s (- 35m 12s) (2000 40%) 74.0313\n",
      "24m 36s (- 33m 59s) (2100 42%) 75.3026\n",
      "25m 48s (- 32m 50s) (2200 44%) 82.8352\n",
      "26m 55s (- 31m 36s) (2300 46%) 75.1532\n",
      "28m 10s (- 30m 31s) (2400 48%) 83.6360\n",
      "29m 21s (- 29m 21s) (2500 50%) 86.0991\n",
      "30m 34s (- 28m 13s) (2600 52%) 85.6719\n",
      "31m 44s (- 27m 2s) (2700 54%) 79.5435\n",
      "32m 57s (- 25m 53s) (2800 56%) 80.8986\n",
      "34m 13s (- 24m 46s) (2900 57%) 82.8373\n",
      "35m 28s (- 23m 38s) (3000 60%) 80.3282\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder2 = EncoderRNN(len(vocab_ingrs.idx2word.keys()), hidden_size).to(device)\n",
    "attn_decoder2 = DecoderRNN(hidden_size, len(vocab_tokens.idx2word.keys())).to(device)\n",
    "\n",
    "trainIters(encoder2, attn_decoder2, 5000, print_every=100,attention=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['cream_cheese', 'sour_cream', 'ground_cumin', 'lime_zest', 'pepper', 'barbecue_sauce', 'tomatoes', 'yellow_peppers', 'green_onions', 'cheddar_cheese', 'tortilla_chips']\n",
      "= [['beat', 'cream', 'cheese', 'with', 'the', 'sour', 'cream', ',', 'cumin', ',', 'lime', 'zest', '.'], ['pepper', 'and', 'half', 'of', 'the', 'diana', 'sauce', 'until', 'smooth', '.'], ['spread', 'into', 'an', '11x7-inch', 'serving', 'dish', '.'], ['toss', 'the', 'remaining', 'diana', 'sauce', 'with', 'tomato', ',', 'yellow', 'pepper', 'and', 'half', 'the', 'green', 'onion', ';', 'spoon', 'evenly', 'over', 'the', 'cream', 'cheese', 'layer', '.'], ['sprinkle', 'with', 'cheddar', 'cheese', 'and', 'the', 'remaining', 'green', 'onions', '.'], ['serve', 'with', 'tortilla', 'chips', '.'], ['tip', ':', 'make', 'your', 'own', 'grilled', 'tortilla', 'chips', 'by', 'cutting', 'flour', 'tortillas', 'into', 'wedges', 'and', 'tossing', 'with', 'a', 'little', 'vegetable', 'oil', '.'], ['grill', 'over', 'medium-low', 'heat', 'until', 'golden', 'and', 'crisp', ',', 'or', 'bake', 'in', 'a', '350f', 'oven', 'until', 'evenly', 'toasted', '.']]\n",
      "< and breast . remove least breast minutes breast breast breast breast breast breast breast breast breast breast breast breast breast breast minutes breast minutes breast breast breast breast breast breast breast minutes breast breast minutes breast minutes breast breast breast breast breast minutes breast breast breast breast minutes breast breast breast breast breast minutes breast breast breast breast breast breast breast minutes breast breast breast breast breast breast breast breast breast minutes breast breast breast breast breast breast breast breast breast breast breast breast breast minutes breast breast minutes breast breast breast breast breast breast breast minutes breast breast breast breast breast minutes breast breast breast breast breast breast minutes breast breast breast breast breast breast breast breast breast breast breast breast minutes breast breast breast minutes breast breast breast breast breast breast breast breast breast minutes breast minutes breast breast breast minutes breast minutes breast breast breast minutes breast breast breast breast breast breast breast breast breast breast breast breast cheese breast minutes breast breast breast breast breast breast breast breast breast breast minutes breast breast breast breast breast breast get breast breast breast minutes breast breast minutes breast minutes breast breast breast minutes breast breast breast breast breast breast breast minutes breast and breast breast breast minutes breast breast breast breast breast minutes breast breast breast breast breast breast breast breast breast breast breast minutes breast breast breast breast breast breast breast breast minutes breast breast minutes breast breast breast breast breast breast breast minutes breast breast breast breast breast breast breast breast breast minutes breast breast minutes breast breast breast breast breast breast breast minutes breast minutes breast breast breast breast breast breast breast breast breast breast breast breast breast minutes breast breast breast breast breast breast breast breast breast breast breast minutes breast breast minutes breast\n",
      "\n",
      "> ['lean_ground_beef', 'fresh_white_breadcrumbs', 'garlic_clove', 'fresh_parsley', 'salt_and_pepper', 'egg', 'mozzarella_cheese', 'vegetable_oil', 'onion', 'tomatoes', 'spaghetti']\n",
      "= [['put', 'the', 'beef', ',', 'breadcrumbs', ',', 'garlic', ',', 'half', 'the', 'parsley', ',', 'saltand', 'pepper', 'into', 'a', 'bowl', '.'], ['add', 'the', 'egg', 'and', 'mix', 'well', '.'], ['cut', 'the', 'mozzarella', 'cheese', 'into', '16', 'small', 'cubes', '.'], ['take', 'a', 'heaped', 'teaspoonful', 'of', 'beef', 'mixture', 'and', 'form', 'it', 'into', 'a', 'ball', 'around', 'each', 'cube', 'of', 'cheese', '.'], ['heat', 'the', 'oil', 'in', 'a', 'large', 'covered', 'pan', ',', 'add', 'the', 'meat', 'balls', 'to', 'the', 'pan', ',', 'cover', 'and', 'cook', 'until', 'brown', 'on', 'all', 'sides', '.'], ['add', 'the', 'onion', 'to', 'the', 'pan', ',', 'cook', 'for', '2', 'to', '3', 'minutes', ',', 'then', 'add', 'the', 'tomatoes', 'and', 'juice', '.'], ['bring', 'up', 'to', 'a', 'simmering', 'point', ',', 'cover', 'and', 'cook', 'for', '30', 'minutes', '.'], ['uncover', 'pan', 'and', 'cook', 'for', 'a', 'further', '10', 'minutes', 'to', 'allow', 'the', 'sauce', 'to', 'reduce', 'and', 'thicken', 'slightly', '.'], ['while', 'the', 'meatballs', 'are', 'cooking', ',', 'cook', 'the', 'spaghetti', 'in', 'boiling', 'water', 'until', 'done', 'to', 'your', 'preference', '.'], ['drain', 'well', 'and', 'divide', 'among', '4', 'warmed', 'plates', '.'], ['divide', 'the', 'sauce', 'among', 'the', 'plates', 'of', 'spaghetti', 'and', 'then', 'sprinkle', 'with', 'the', 'remaining', 'parsley', '.']]\n",
      "< preheat and . <EOS>\n",
      "\n",
      "> ['low_-_fat_mayonnaise', 'orange_juice_concentrate', 'lime_juice', 'ground_cumin', 'hot_sauce', 'boneless_skinless_chicken_breasts', 'sourdough_bread', 'romaine_lettuce', 'tomatoes', 'avocado']\n",
      "= [['mix', 'together', 'the', 'mayo', ',', 'orange', 'juice', 'concentrate', ',', 'lime', 'juice', ',', 'cumin', ',', 'and', 'hot', 'sauce', 'in', 'a', 'small', 'bowl', '.'], ['set', 'aside', '.'], ['season', 'chicken', 'breasts', 'with', 'salt', 'and', 'pepper', '.'], ['cook', 'in', 'a', 'nonstick', 'skillet', 'coated', 'with', 'cooking', 'spray', 'over', 'a', 'medium-high', 'heat', 'for', '5', 'minutes', 'on', 'each', 'side', '.'], ['reduce', 'heat', 'to', 'low', '.'], ['cover', ',', 'cook', 'for', '5', 'more', 'minutes', 'or', 'until', 'done', '.'], ['remove', 'from', 'heat', 'and', 'cut', 'diagonally', 'across', 'the', 'grain', 'into', 'thin', 'slices', '.'], ['spread', '1', 'tablespoon', 'citrus', 'mayonnaise', 'on', 'each', 'of', 'the', 'four', 'bread', 'slices', '.'], ['top', 'with', 'lettuce', ',', 'chicken', ',', 'tomatoes', ',', 'avocado', ',', 'and', 'remaining', 'bread', 'slice', '.']]\n",
      "< preheat salt fold cheese honey lettuce salt fold cheese honey lettuce fold cheese honey lettuce <EOS>\n",
      "\n",
      "> ['apricot_preserves', 'soy_sauce', 'chicken_broth', 'vegetable_oil', 'cornstarch', 'garlic', 'ground_ginger', 'boneless_skinless_chicken_breasts', 'green_pepper', 'salted_cashews', 'cooked_rice']\n",
      "= [['in', 'a', 'large', 'bowl', ',', 'combine', 'the', 'first', 'seven', 'ingredients', '.'], ['add', 'chicken', 'and', 'toss', 'to', 'coat', '.'], ['transfer', 'to', 'a', 'shallow', 'microwave-safe', 'dish', '.'], ['cover', 'and', 'microwave', 'on', 'high', 'for', '3', 'minutes', ',', 'stirring', 'once', '.'], ['add', 'green', 'pepper', 'and', 'cashews', '.'], ['cover', 'and', 'microwave', 'on', 'high', 'for', '2-4', 'minutes', 'or', 'until', 'chicken', 'juices', 'run', 'clear', ',', 'stirring', 'once', '.'], ['let', 'stand', 'for', '3', 'minutes', '.'], ['serve', 'with', 'rice', '.'], ['editor', \"'s\", 'note', ':', 'this', 'recipe', 'was', 'tested', 'in', 'a', '1,100-watt', 'microwave', '.']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< preheat honey honey honey honey honey juice and honey and honey juice water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes water minutes\n",
      "\n",
      "> ['eggs', 'vegetable_oil', 'beer', 'all_-_purpose_flour', 'assorted_herbs', 'coarse_salt', 'fresh_ground_pepper', 'lemon']\n",
      "= [['whisk', 'together', 'the', 'yolks', ',', 'oil', ',', 'and', 'beer', 'in', 'a', 'bowl', '.'], ['slowly', 'add', 'the', 'flour', ',', 'whisking', 'until', 'just', 'combined', '.'], ['set', 'aside', 'for', '20', 'minutes', '.'], ['wash', 'and', 'dry', 'the', 'herbs', '.'], ['whisk', 'the', 'egg', 'whites', 'to', 'soft', 'peaks', ';', 'fold', 'into', 'the', 'beer', 'batter', '.'], ['add', 'the', 'salt', 'and', 'pepper', '.'], ['in', 'a', 'heavy-bottomed', 'saucepan', ',', 'heat', '1', 'inch', 'oil', 'over', 'medium', 'heat', 'until', 'hot', 'but', 'not', 'smoking', ',', 'about', '375f', 'on', 'a', 'fry', 'thermometer', '.'], ['dip', 'each', 'herb', 'into', 'the', 'batter', ',', 'shaking', 'off', 'excess', ',', 'until', 'lightly', 'coated', '.'], ['place', 'the', 'herbs', 'in', 'the', 'oil', ',', 'turning', 'until', 'golden', ',', 'about', '1', 'minute', '.'], ['drain', 'on', 'paper', 'towels', ';', 'season', 'with', 'salt', '.'], ['serve', 'with', 'lemon', 'wedges', '.']]\n",
      "< bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir bread stir\n",
      "\n",
      "> ['butter', 'rolled_oats', 'sugar', 'egg', 'vanilla', 'all_-_purpose_flour', 'salt', 'baking_powder']\n",
      "= [['line', 'a', 'cookie', 'sheet', 'with', 'foil', ';', 'lightly', 'butter', 'foil', '.'], ['set', 'aside', '.'], ['in', 'a', 'large', 'saucepan', ',', 'melt', 'butter', '.'], ['remove', 'saucepan', 'from', 'heat', '.'], ['stir', 'in', 'rolled', 'oats', ',', 'sugar', ',', 'egg', ',', 'and', 'vanilla', '.'], ['add', 'flour', ',', 'salt', ',', 'and', 'baking', 'powder', ';', 'mix', 'well', '.'], ['drop', 'rounded', 'teaspoons', 'of', 'cookie', 'batter', 'about', '3', 'inches', 'apart', 'onto', 'foil', '(', 'about', '9', 'cookies', 'on', 'a', 'medium', 'cookie', 'sheet', ')', '.'], ['spread', 'each', 'dollop', 'of', 'cookie', 'batter', 'slightly', 'to', 'distribute', 'the', 'oats', '.'], ['bake', 'in', 'a', '350', 'degree', 'f', 'oven', 'for', '9', 'to', '11', 'minutes', 'or', 'until', 'golden', 'brown', '.'], ['remove', 'from', 'the', 'oven', ';', 'let', 'cool', 'on', 'cookie', 'sheets', 'on', 'a', 'wire', 'rack', 'about', '10', 'minutes', 'or', 'until', 'the', 'cookies', 'are', 'set', '.'], ['remove', 'from', 'the', 'foil', '.']]\n",
      "< preheat mixture . remove and cut and cut and cut and cut and cut and cut and cut get get get get get get get get get minutes get get get get get get get get get minutes get get get get get get get minutes get get get get get get get mixture except get baking get get not remove mixture except get not remove mixture except get not remove mixture get not remove mixture get baking get baking get not remove mixture get not remove mixture except baking get not remove mixture except get not remove mixture get baking get not remove mixture get not remove mixture get not remove mixture except minutes get baking get not remove mixture except mixture get not remove mixture get not remove mixture get not remove mixture except get baking get not remove mixture except mixture get baking get not remove mixture except get baking get not remove mixture get baking get baking get not remove mixture get not remove mixture except baking get not remove mixture get not remove mixture get baking get not remove mixture get get baking get not remove mixture get not remove mixture get not remove mixture get not remove mixture except get not remove mixture get baking get not remove mixture get baking get not remove mixture except get not remove mixture except get baking get not remove mixture get not remove mixture get not remove mixture get baking get not remove mixture get not remove mixture get not remove mixture except baking get baking get not remove mixture get not remove mixture except mixture get not remove mixture except baking get not remove mixture get not remove mixture get mixture except get not remove mixture get not remove mixture get baking get not remove mixture\n",
      "\n",
      "> ['seasoned_croutons', 'pork_sausage', 'eggs', 'low_-_fat_milk', 'mushroom_soup', 'sliced_mushrooms', 'dry_mustard', 'cheddar_cheese']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= [['spread', 'croutons', 'in', 'a', 'lightly', 'greased', '13', 'x', '9', 'x', '2', 'inch', 'pan', '.'], ['set', 'aside', '.'], ['cook', 'sausage', ',', 'drain', '.'], ['crumble', '.'], ['sprinkle', 'sausage', 'over', 'croutons', '.'], ['combine', 'egg', ',', 'lowfat', 'milk', ',', 'soup', ',', 'mushrooms', ',', 'and', 'dry', 'mustard', '-', 'mix', 'well', 'and', 'pour', 'over', 'sausage', '.'], ['cover', 'and', 'chill', 'overnight', 'or', 'possibly', 'at', 'least', '8', 'hrs', '.'], ['remove', 'from', 'refrigerator', '-', 'let', 'stand', '30', 'min', '.'], ['bake', 'uncovered', 'at', '325', 'degrees', 'for', '50', 'to', '55', 'min', '.'], ['sprinkle', 'cheese', 'over', 'top', 'and', 'bake', 'an', 'additional', '5', 'min', 'or', 'possibly', 'till', 'cheese', 'melts', '.']]\n",
      "< and . grill and salt and and and and and honey with salt honey salt honey salt honey salt honey salt honey honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt . honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt . stir with salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey honey salt honey salt honey honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt with salt honey salt honey honey salt honey salt honey salt honey salt . honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt with salt honey salt honey salt honey salt honey salt honey honey salt honey salt honey salt honey salt honey salt honey honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey honey salt honey salt honey honey salt honey salt honey salt honey salt honey salt honey salt with salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey honey honey salt honey salt honey honey honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey salt honey\n",
      "\n",
      "> ['sirloin', 'fresh_broccoli_florets', 'onion', 'garlic', 'cornstarch', 'low_sodium_soy_sauce', 'cooking_sherry', 'honey', 'vegetable_oil', 'crushed_red_pepper_flakes']\n",
      "= [['cut', 'onion', 'and', 'beef', 'into', 'thin', 'strips', '(', 'beef', 'should', 'be', 'bite', 'size', ')', '.'], ['add', 'two', 'tbl', 'oil', 'to', 'hot', 'pan', 'or', 'wok', '.'], ['when', 'oil', 'is', 'hot', 'and', 'swirly', ',', 'add', 'garlic', 'and', 'onions', '.'], ['cook', 'two', 'minutes', 'or', 'until', 'onion', 'is', 'lightly', 'browned', '.'], ['add', 'florets', 'and', 'cook', 'two', 'minutes', '.'], ['add', 'sirloin', 'and', 'stir', 'fry', 'until', 'cooked', 'through', '.'], ['do', \"n't\", 'overcook', 'or', 'florets', 'will', 'be', 'mushy', '!'], ['meanwhile', ',', 'combine', 'sherry', ',', 'cornstarch', 'and', 'soy', '.'], ['turn', 'heat', 'off', 'and', 'drizzle', 'honey', 'all', 'over', 'stir', 'fry', '.'], ['add', 'soy', 'mixture', 'and', 'stir', '.'], ['stir', 'in', 'red', 'pepper', '.']]\n",
      "< juice cut juice get vegetable mixture get vegetable . get vegetable . get vegetable . juice get vegetable . get vegetable . get vegetable . get vegetable get vegetable get vegetable . get vegetable . get vegetable . get vegetable get vegetable . get vegetable . get vegetable . get vegetable . get vegetable get vegetable . get vegetable . get vegetable . get vegetable . get vegetable . get vegetable . get vegetable . get vegetable . get vegetable . get well get vegetable . get vegetable . get vegetable . get vegetable . get vegetable . get juice get vegetable . get vegetable get vegetable . get vegetable . remove get juice get vegetable get get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get juice get vegetable get vegetable get get vegetable get vegetable get vegetable get vegetable get vegetable get get vegetable get vegetable get get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get juice get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get juice get vegetable get get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get vegetable get juice get vegetable get vegetable get vegetable get vegetable get vegetable get juice get vegetable get vegetable get\n",
      "\n",
      "> ['pie_pastry', 'swiss_cheese', 'green_onions', 'eggs', 'half_-_and_-_half_cream', 'pimientos', 'salt', 'cayenne_pepper', 'parmesan_cheese']\n",
      "= [['heat', 'oven', 'to', '375', 'degrees', '.'], ['spray', '24', 'mini', 'muffin', 'tins', 'with', 'nonstick', 'cooking', 'spray', '.'], ['roll', 'out', 'the', 'dough', ',', 'then', 'cut', 'into', '24-26', '2-1/2-inch', 'rounds', '(', 'you', 'might', 'get', 'less', ')', '.'], ['press', 'one', 'round', 'into', 'the', 'bottom', 'and', 'sides', 'of', 'each', 'mini', 'muffin', 'tin', '.'], ['place', 'about', '1', 'tablespoon', 'of', 'shredded', 'cheese', 'in', 'the', 'bottom', 'of', 'the', 'crust', '.'], ['top', 'with', 'a', 'few', 'green', 'onions', ',', 'and', 'then', 'some', 'chopped', 'pimientos', '.'], ['in', 'a', 'small', 'bow', ',', 'whisk', 'together', 'the', 'eggs', ',', 'half', 'and', 'half', 'cream', ',', 'salt', ',', 'cayenne', 'and', 'parmesan', 'cheese', '.'], ['divide', 'the', 'mixture', 'between', 'each', 'muffin', 'tin', 'to', 'within', '1/4', 'inch', 'from', 'the', 'top', '.'], ['bake', '25-30', 'minutes', 'or', 'until', 'golden', 'brown', '.'], ['cool', '3', 'minutes', 'then', 'carefully', 'run', 'a', 'knife', 'around', 'the', 'quiches', 'and', 'lift', 'out', '.'], ['serve', 'warm', '.']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< and salt . get mixture cheese thoroughly thoroughly get cheese get cheese turkey cheese get cheese thoroughly thoroughly thoroughly cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese thoroughly cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese cheese get cheese get cheese get cheese get cheese get cheese turkey cheese turkey cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese thoroughly cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese turkey cheese get cheese turkey cheese get cheese turkey cheese get cheese get cheese get cheese get cheese thoroughly cheese get cheese thoroughly thoroughly cheese get cheese get cheese get cheese get cheese get cheese get cheese turkey cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese turkey cheese get cheese get cheese get cheese get cheese turkey cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese turkey cheese get cheese get cheese turkey cheese get cheese get cheese get cheese turkey cheese thoroughly cheese get cheese get cheese get cheese get cheese get cheese turkey cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese turkey cheese get cheese get cheese thoroughly get cheese get cheese get cheese thoroughly cheese get cheese turkey cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese get cheese turkey cheese get cheese get\n",
      "\n",
      "> ['sweet_potatoes', 'unsweetened_frozen_peach_slices', 'butter', 'lemon_juice', 'brown_sugar', 'ground_ginger', 'salt', 'coffee_-_flavored_liqueur', 'pecans']\n",
      "= [['preheat', 'oven', 'to', '350', 'degrees', 'f', '(', '175', 'degrees', 'c', ')', '.'], ['lightly', 'grease', 'a', '9x13-inch', 'baking', 'dish', '.'], ['arrange', 'half', 'the', 'sweet', 'potatoes', 'in', 'the', 'bottom', 'of', 'the', 'prepared', 'baking', 'dish', '.'], ['layer', 'with', 'half', 'of', 'the', 'peach', 'slices', 'and', 'dot', 'with', 'half', 'the', 'butter', '.'], ['repeat', 'layering', 'with', 'remaining', 'sweet', 'potatoes', 'and', 'peach', 'slices', 'and', 'sprinkle', 'with', 'lemon', 'juice', '.'], ['combine', 'the', 'brown', 'sugar', ',', 'ginger', ',', 'and', 'salt', 'and', 'mix', 'well', '.'], ['sprinkle', 'the', 'sugar', 'mixture', 'over', 'the', 'potatoes', 'and', 'peaches', '.'], ['dot', 'the', 'casserole', 'with', 'remaining', 'butter', ',', 'and', 'pour', 'in', 'the', 'liqueur', '(', 'see', 'cook', \"'s\", 'note', ')', '.'], ['cover', 'pan', 'with', 'aluminum', 'foil', '.'], ['bake', 'in', 'the', 'preheated', 'oven', 'until', 'sweet', 'potatoes', 'are', 'tender', ',', 'about', '1', 'hour', '.'], ['remove', 'foil', ',', 'sprinkle', 'with', 'pecans', ',', 'and', 'continue', 'baking', '10', 'to', '15', 'minutes', ',', 'until', 'pecans', 'are', 'toasted', 'and', 'fragrant', '.']]\n",
      "< stirring ; get . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 ; get stirring 4 . stir 4 . stir 4 . stir 4 . stir 4 ; get stirring 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 ; get stirring 4 . stir 4 . stir 4 ; get . stir 4 ; get stirring 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 ; get thoroughly celery 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 ; get thoroughly celery 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 ; get . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 . stir 4 ; get stirring 4 . stir\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Attention\n",
    "---------------------\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
    "displayed as a matrix, with the columns being input steps and rows being\n",
    "output steps:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better viewing experience we will do the extra work of adding axes\n",
    "and labels:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "\n",
    "evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "\n",
    "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "=========\n",
    "\n",
    "-  Replace the embeddings with pre-trained word embeddings such as word2vec or\n",
    "   GloVe\n",
    "-  Try with more layers, more hidden units, and more sentences. Compare\n",
    "   the training time and results.\n",
    "-  If you use a translation file where pairs have two of the same phrase\n",
    "   (``I am test \\t I am test``), you can use this as an autoencoder. Try\n",
    "   this:\n",
    "\n",
    "   -  Train as an autoencoder\n",
    "   -  Save only the Encoder network\n",
    "   -  Train a new Decoder for translation from there\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
